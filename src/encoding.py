from util.packing import unpack_ints, pack_ints
from util.randoms import random_ints
from util.lists import take, to_generator
from util.modeling import tabulate, recite
from util.padding import pad, unpad

def encode(model, text, block_size=16):
    """Encodes a list of values into a list of approximately uniformly random
    weights as determined by the model's predictions for each value in
    `values`.

    The higher the model's prediction accuracy, the more uniformly random the
    output weights will be.

    The length of the encoded bytes will be a multiple of `block_size`.

    Example:
        >> decode(model, encode(model, "foobar", 16))
        "FOOBAR "

    Args:
        model (Model): The model to use for encoding. This should be trained on
            a domain related to `text`.

        text (string): The text to encode.

        block_size (int, optional): The output will be padded to be a multiple
            of `block_size`.

    Returns (bytes):
        A byte array containing the weights used to encode the text.

    Raises:
        ValueError: If `text` contains an item that isn't in the `model`'s
            alphabet.

        Exception: If padding the encoded plaintext fails. This is a
            non-deterministic process. The probability of this happening is
            highly unlikely, but not impossible. If your model has a boundary
            that occurs with a low-probability and you're getting this
            exception, increase your model's max_padding_trials attribute.
    """
    randoms = random_ints() # Infinite stream of random ints
    (initial_weights, initial_sequence) = _initialize(model, randoms)

    transformed = list(model.transform(text))
    padded = pad(model, initial_sequence, transformed, block_size)
    encoded = tabulate(model, initial_sequence, padded)
    return pack_ints(initial_weights + list(encoded))

def decode(model, data):
    """Decodes a byte array of weights into a string that is generated by
    using these weights to choose characters from a model's probability
    distribution.

    Example:
        >> decode(model, encode(model, "foobar", 16))
        "FOOBAR "

    Args:
        model (Model): The model that was used when encoding the provided data.

        data (bytes): The data containing the encoded weights to be used for
            decoding.

    Returns (string):
        The decoded string.
    """
    randoms = to_generator(unpack_ints(data))
    (_, initial_sequence) = _initialize(model, randoms)

    decoded = recite(model, initial_sequence, randoms)
    unpadded = unpad(model, list(decoded))
    return ''.join(unpadded)

def _initialize(model, randoms):
    """Given a model, returns the initial sequence to use for encoding, along
    with the weights that were used to generate that sequence.

    This initial sequence is generated after 1) normalizing the model and 2)
    priming the sequence.

    What does normalizing do?
    -------------------------

    The initial (seed) sequence in the model is entirely random
    (see: `_seed_sequence`), which results in the first few predictions of the
    model having a skewed disitribution that isn't representative of the
    underlying domain.

    What does priming do?
    ---------------------

    We want the initial sequence to be unique and representative of the
    underlying domain. We also want to start encoding after a boundary
    character. Priming generates a sequence and then chops off the suffix until
    a boundary is hit. This ensures that we don't start encoding with an
    initial sequence that end in the middle of a token.

    That is, if our priming step resulted in:

        "ATTACK THE EASTERN FRO"

    We'd return:

        "ATTACK THE EASTERN "

    If we didn't end on a boundary, as in the first example, then the model's
    predictions will be *really* skewed for the first few characters we encode.

    Example:
        >> from random import randint
        >> length = model.sequence_length + model.normalizing_length + model.priming_length
        >> randoms = [randint(0, 2**32 - 1) for _ in range(length)]
        >>
        >> (weights, sequence) = _initialize(model, to_generator(randoms))
        >>
        >> "".join(sequence)
        'ESPONSIBLE FOR PROVIDING THE READINESS ACTIVITIES '
        >> weights == list(randoms)
        True

    Args:
        model (Model): The model to use for generating the initial sequence.

        randoms (generator<int>): A generator that returns a sequence of
            32-bit integers.

    Returns ((list(ints), list(char))):
        A tuple containing the list of weights used to generate the sequence,
        and the sequence itself.
    """
    seed = take(model.sequence_length, randoms)
    normals = take(model.normalizing_length, randoms)
    priming = take(model.priming_length, randoms)

    start = _seed_sequence(model, seed)
    primed = list(recite(model, start, normals + priming))
    unpadded = unpad(model, primed) # Removes any partial tokens at end
    return (seed + normals + priming, start + unpadded[-model.sequence_length:])

def _seed_sequence(model, seed):
    """Generates a uniformly random sequence drawn from the model's alphabet.

    Note: If the length of the model's alphabet is not a power of 2 then there
    will be a small and neglible amount of skew towards earlier characters in
    the alphabet.

    This skew is bounded by:
        1 + (1 / floor(2^32 / alphabet_length))

    That results in a maximum skew of 1.0000002 for a model with an alphabet
    of 1,000 or fewer characters.

    The randomness of this seed is not critical for security. That is, this
    function could return a fixed seed without impacting the security of the
    system.

    Example:
        >> from random import randint
        >> _seed_sequence(model, (randint(0, 2**32 - 1) for _ in range(10))
        ['R', 'U', 'B', 'S', 'T', 'K', '7', 'P', '5', 'A']

    Args:
        model (Model): The model that you're generating a seed for.

        seed (list(int)): The random 32-bit integers to use to generate the
            sequence.

    """
    alphabet = model.alphabet
    alphabet_size = len(alphabet)
    return [alphabet[r % alphabet_size] for r in seed]
